{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#載入套件\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import chi2_contingency\n",
    "import math\n",
    "import uuid\n",
    "import random\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 分割x and y\n",
    "def split_x_y(data_train,data_test,target=\"kredit\"):\n",
    "\n",
    "  train_y = data_train[target]\n",
    "  train_x = data_train.drop(target, axis=1)\n",
    "\n",
    "  test_x = data_test.drop(target, axis=1)\n",
    "  test_y = data_test[target]\n",
    "\n",
    "  return train_x, train_y, test_x, test_y\n",
    "\n",
    "# \n",
    "def add_id(original_df):\n",
    "  id = pd.Series(range(0,1000)).apply(lambda i : str(uuid.uuid4()))\n",
    "  df_with_id = original_df.copy()\n",
    "  df_with_id['id'] = id\n",
    "  df_with_id = df_with_id.set_index('id')\n",
    "\n",
    "  return df_with_id\n",
    "\n",
    "# def split_columns(df_with_id,columns_list,target):\n",
    "#   colum_1 = columns_list.copy()\n",
    "#   colum_1.append(target)\n",
    "#   colum_2 = columns_list.copy()\n",
    "  \n",
    "#   # 分割欄位\n",
    "#   client1_data = df_with_id[colum_1]\n",
    "#   client2_data = df_with_id.drop(colum_2, axis=1)\n",
    "\n",
    "#   # 切割資料\n",
    "#   client1_train, client1_test = train_test_split(client1_data, test_size=0.2)\n",
    "#   client2_train, client2_test = train_test_split(client2_data, test_size=0.2)\n",
    "\n",
    "#   # 切割x,y\n",
    "#   client1_train_x,client1_train_y, client1_test_x, client1_test_y = split_x_y(client1_train,client1_test,target)\n",
    "#   client2_train_x,client2_train_y, client2_test_x, client2_test_y = split_x_y(client2_train,client2_test,target)\n",
    "\n",
    "#   # 找出共同的index\n",
    "#   common_train_index = client1_train.index.intersection(client2_train.index)\n",
    "#   common_test_index = client1_test.index.intersection(client2_test.index)\n",
    "\n",
    "#   return client1_train_x,client1_train_y, client1_test_x, client1_test_y, client2_train_x,client2_train_y, client2_test_x, client2_test_y, common_train_index, common_test_index\n",
    "def split_columns(df_with_id,columns_list,target):\n",
    "  colum_1 = columns_list.copy()\n",
    "  colum_1.append(target)\n",
    "  colum_2 = columns_list.copy()\n",
    "  \n",
    "  # 分割資料\n",
    "  train_data, test_data = train_test_split(df_with_id, test_size=0.2)\n",
    "  \n",
    "  # 分割欄位\n",
    "  client1_train = train_data[colum_1]\n",
    "  client1_test = test_data[colum_1]\n",
    "  \n",
    "  client2_train = train_data.drop(colum_2, axis=1)\n",
    "\n",
    "  client2_test = test_data.drop(colum_2, axis=1)\n",
    "  \n",
    "  # 切割x,y\n",
    "  client1_train_x,client1_train_y, client1_test_x, client1_test_y = split_x_y(client1_train,client1_test,target)\n",
    "  client2_train_x,client2_train_y, client2_test_x, client2_test_y = split_x_y(client2_train,client2_test,target)\n",
    "\n",
    "  # 找出共同的index\n",
    "  common_train_index = client1_train.index.intersection(client2_train.index)\n",
    "  common_test_index = client1_test.index.intersection(client2_test.index)\n",
    "\n",
    "  return client1_train_x,client1_train_y, client1_test_x, client1_test_y, client2_train_x,client2_train_y, client2_test_x, client2_test_y, common_train_index, common_test_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model\n",
    "# normalize data\n",
    "def normalize_data(data):\n",
    "  normalizer = tf.keras.layers.Normalization()\n",
    "  normalizer.adapt(np.array(data))\n",
    "  return normalizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#client\n",
    "class Client:\n",
    "\n",
    "  def __init__(self, train_data_x,train_data_y, test_data_x,test_data_y, labelled,model):\n",
    "    self.__trainX = train_data_x.copy()\n",
    "    self.__testX = test_data_x.copy()\n",
    "    self.labelled = labelled\n",
    "    self.__trainY = train_data_y.copy()\n",
    "    self.__testY = test_data_y.copy()\n",
    "    self.model = model\n",
    "    \n",
    "  def next_batch(self, index):\n",
    "    self.batchX = self.__trainX.loc[index]\n",
    "\n",
    "    if not self.labelled:\n",
    "      grads = []\n",
    "      self.model_output = np.zeros((len(index), 2))\n",
    "      for i in range(len(index)):\n",
    "        with tf.GradientTape() as gt:\n",
    "          gt.watch(self.model.trainable_weights)\n",
    "          output_by_example = self.model(self.batchX.iloc[i:i+1], training=True)\n",
    "          output_for_grad = output_by_example[:,1]\n",
    "        self.model_output[i] = output_by_example\n",
    "        grads.append(gt.gradient(output_for_grad, self.model.trainable_weights))\n",
    "\n",
    "      return grads\n",
    "    \n",
    "    else:\n",
    "      self.batchY = self.__trainY.loc[index]\n",
    "      with tf.GradientTape() as self.gt:\n",
    "        self.gt.watch(self.model.trainable_weights)\n",
    "        self.model_output = self.model(self.batchX, training=True)\n",
    "  def cal_model(self):\n",
    "    return self.model_output\n",
    "  \n",
    "  def predict(self, test_index):\n",
    "    return self.model.predict(self.__testX.loc[test_index])# + 1e-8\n",
    "\n",
    "  def predict_all(self, index):\n",
    "    return self.model.predict(pd.concat([self.__trainX, self.__testX]).loc[index])\n",
    "\n",
    "  def test_answers(self, test_index):\n",
    "    if self.labelled:\n",
    "      return self.__testY.loc[test_index]\n",
    "    \n",
    "  def test_answers_all(self, index):\n",
    "    if self.labelled:\n",
    "      return pd.concat([self.__testY, self.__trainY]).loc[index]\n",
    "  \n",
    "  def batch_answers(self):\n",
    "    if self.labelled:\n",
    "      return self.batchY\n",
    "\n",
    "  def loss_and_update(self, a):\n",
    "    if not self.labelled:\n",
    "      raise AssertionError(\"This method can only be called by client 2\")\n",
    "    self.prob = (a + self.model_output)/2\n",
    "    self.c = self.coefficient_and_update()/len(self.batchX)\n",
    "    return self.prob, loss_fn(self.batchY, self.prob)\n",
    "  \n",
    "  def coefficient_and_update(self):\n",
    "    if not self.labelled:\n",
    "      raise AssertionError(\"This method can only be called by client 2\")\n",
    "    p = self.prob[:,1]\n",
    "    c = (p-self.batchY)/((p)*(1-p))\n",
    "    with self.gt:\n",
    "      output = sum(c * self.model_output[:,1])/len(c)\n",
    "    grads = self.gt.gradient(output, self.model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "    return c\n",
    "  \n",
    "  def update_with(self, grads):\n",
    "    optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "\n",
    "  def assemble_grad(self, partial_grads):\n",
    "    if not self.labelled:\n",
    "      raise AssertionError(\"This method can only be called by client 2\")\n",
    "    # to assemble the gradient for client 1\n",
    "    for i in range(len(self.c)):\n",
    "      partial_grads[i] = [x * self.c[i] for x in partial_grads[i]]\n",
    "    return [sum(x) for x in zip(*partial_grads)]\n",
    "# 畫圖\n",
    "# roc curve\n",
    "def draw_roc_curve(fpr, tpr,auc):\n",
    "  fig, ax = plt.subplots()\n",
    "  plt.title('Receiver Operating Characteristic')\n",
    "  plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n",
    "  plt.legend(loc = 'lower right')\n",
    "  plt.plot([0, 1], [0, 1],'r--')\n",
    "  plt.xlim([0, 1])\n",
    "  plt.ylim([0, 1])\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.show()  \n",
    "\n",
    "\n",
    "# 訓練圖\n",
    "def plot_loss(loss, accuracy):\n",
    "  fig, ax = plt.subplots()\n",
    "  plt.plot(loss, label='loss')\n",
    "  plt.plot(accuracy, label='accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  # plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "# 準確率圖\n",
    "def plot_accuracy(predictions, answers, threshold):\n",
    "  tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "  for x in range(len(predictions)):\n",
    "    if answers[x] == 1:\n",
    "      # if np.argmax(predictions[x]) == 1:\n",
    "      if predictions[x][1] >= threshold:\n",
    "        tp = tp + 1\n",
    "      else:\n",
    "        fn = fn + 1\n",
    "    else:\n",
    "      # if np.argmax(predictions[x]) == 0:\n",
    "      if predictions[x][1] < threshold:\n",
    "        tn = tn + 1\n",
    "      else:\n",
    "        fp = fp + 1\n",
    "  \n",
    "  accuracy = (tp + tn)/(tp + fp + fn + tn)\n",
    "  precision = tp / (tp + fp)\n",
    "  recall = tp / (tp + fn)\n",
    "  specificity = tn / (tn + fp)\n",
    "  \n",
    "  fmeasure = 2*(recall * precision) / (recall + precision)\n",
    "  print(\"Accuracy: \" + str(accuracy))\n",
    "  print(\"Precision: \" + str(precision))\n",
    "  print(\"Recall: \" + str(recall))\n",
    "  # print(\"Specificity: \" + str(specificity))\n",
    "  print(\"F-Measure: \" + str(2*(recall * precision) / (recall + precision)))\n",
    "  \n",
    "  return accuracy, precision, recall, fmeasure"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 下載資料\n",
    "\n",
    "# detect SouthGermanCredit file is exist or not\n",
    "if not os.path.exists('./SouthGermanCredit'):\n",
    "  !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00573/SouthGermanCredit.zip\n",
    "\n",
    "else:\n",
    "  print('SouthGermanCredit file is exist')\n",
    "  \n",
    "with zipfile.ZipFile('SouthGermanCredit.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./SouthGermanCredit/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 清理資料\n",
    "original_df = pd.read_csv('./SouthGermanCredit/SouthGermanCredit.asc', sep=' ')\n",
    "original_df.describe()\n",
    "original_df=original_df.dropna()\n",
    "\n",
    "\n",
    "# Normalize all columns\n",
    "scaler = MinMaxScaler()\n",
    "df_norm = scaler.fit_transform(original_df)\n",
    "original_df =pd.DataFrame(df_norm,columns=original_df.columns)\n",
    "\n",
    "# choose target\n",
    "target='kredit'\n",
    "columns_list = ['rate',\t'famges',\t'buerge',\t'wohnzeit',\t'verm',\t'alter','laufkont','moral']\n",
    "# 添加id\n",
    "df_with_id = add_id(original_df)\n",
    "# 切割資料\n",
    "client1_train_x,client1_train_y, client1_test_x, client1_test_y, client2_train_x,client2_train_y, client2_test_x, client2_test_y, common_train_index, common_test_index = split_columns(df_with_id,columns_list=columns_list,target=target)\n",
    "# 顯示訓練測試資料大小\n",
    "print(\n",
    "    'There are {} common entries (out of {}) in client 1 and client 2\\'s training datasets,\\nand {} common entries (out of {}) in their test datasets'\n",
    "    .format(\n",
    "        len(common_train_index),\n",
    "        len(client1_train_x),\n",
    "        len(common_test_index),\n",
    "        len(client1_test_x)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#vfl\n",
    "# 設定參數\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "num_folds = 20\n",
    "\n",
    "# merge train and test data\n",
    "client1_input = pd.concat((client1_train_x,client1_test_x),axis=0)\n",
    "client2_input = pd.concat((client2_train_x,client2_test_x),axis=0)\n",
    "client1_target = pd.concat((client1_train_y,client1_test_y),axis=0)\n",
    "client2_target = pd.concat((client2_train_y,client2_test_y),axis=0)\n",
    "\n",
    "# # get index\n",
    "# client1_input_index = client1_input.index.to_numpy()\n",
    "# client2_input_index = client2_input.index.to_numpy()\n",
    "# client1_target_index = client1_target.index.to_numpy()\n",
    "# client2_target_index = client2_target.index.to_numpy()\n",
    "\n",
    "# common trans to numpy\n",
    "common_train_index_k = common_train_index.to_numpy()\n",
    "common_test_index = common_test_index.to_numpy()\n",
    "cv_common_index = np.concatenate((common_train_index_k,common_test_index),axis=0)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "# Instantiate a loss function.\n",
    "# Not from logits because of the softmax layer converting logits to probability.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# Instantiate a metric function (accuracy)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# define the kfold cross validation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# kfold cross validation evaluation of a model\n",
    "\n",
    "df_result = pd.DataFrame(columns=['model','fold','accuracy','precision','recall','fmeasure'])\n",
    "\n",
    "fold_size = len(cv_common_index ) // num_folds\n",
    "for i in range(num_folds):\n",
    "  test_start = i * fold_size\n",
    "  test_end = test_start + fold_size\n",
    "  \n",
    "  test_common_index_k = cv_common_index[test_start:test_end]\n",
    "  \n",
    "  train_common_index_k = np.concatenate((cv_common_index[:test_start], cv_common_index[test_end:]))\n",
    "  \n",
    "\n",
    "  print(f'------this is {i}/{num_folds} fold------')\n",
    "  # index\n",
    "  cen1_train_x_index = train_common_index_k\n",
    "  cen2_train_x_index = train_common_index_k\n",
    "  \n",
    "  cen1_test_x_index = test_common_index_k\n",
    "  cen2_test_x_index = test_common_index_k\n",
    "  \n",
    "  cen1_train_y_index = train_common_index_k\n",
    "  cen2_train_y_index = train_common_index_k\n",
    "  \n",
    "  cen1_test_y_index = test_common_index_k\n",
    "  cen2_test_y_index = test_common_index_k\n",
    "  \n",
    "  \n",
    "  # define train,test x use index\n",
    "  client1_train_x_k = client1_input[client1_input.index.isin(cen1_train_x_index)]\n",
    "  client2_train_x_k = client2_input[client2_input.index.isin(cen2_train_x_index)]\n",
    "  \n",
    "  client1_test_x_k = client1_input[client1_input.index.isin(cen1_test_x_index)]\n",
    "  client2_test_x_k = client2_input[client2_input.index.isin(cen2_test_x_index)]\n",
    "\n",
    "  client1_train_y_k = client1_target[client1_target.index.isin(cen1_train_y_index)]\n",
    "  client2_train_y_k = client2_target[client2_target.index.isin(cen2_train_y_index)]\n",
    "  \n",
    "  client1_test_y_k = client1_target[client1_target.index.isin(cen1_test_y_index)]\n",
    "  client2_test_y_k = client2_target[client2_target.index.isin(cen2_test_y_index)]\n",
    "  \n",
    "\n",
    "  common_train_index_k = np.intersect1d(cen1_train_x_index,cen2_train_x_index)\n",
    "  common_test_index_k = np.intersect1d(cen1_test_x_index,cen2_test_x_index)\n",
    "  \n",
    "  \n",
    "  #init model\n",
    "  normalizer1 = normalize_data(client1_train_x_k.loc[common_train_index_k])\n",
    "  model1 =   tf.keras.Sequential([\n",
    "        normalizer1,\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2),\n",
    "        layers.Softmax()])\n",
    "  client1 = Client( client1_train_x_k, client1_train_y_k,client1_test_x_k,client1_test_y_k, False,model1)\n",
    "\n",
    "\n",
    "  normalizer2 = normalize_data(client2_train_x_k.loc[common_train_index_k])\n",
    "  model2 = tf.keras.Sequential([\n",
    "        normalizer2,\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2),\n",
    "        layers.Softmax()])\n",
    "  client2 = Client(client2_train_x_k, client2_train_y_k,client2_test_x_k,client2_test_y_k, True,model2)\n",
    "\n",
    "  # train_on_client\n",
    "  common_train_index_list_k = common_train_index_k.tolist()\n",
    "  epoch_loss = []\n",
    "  epoch_acc = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "      print(f'run in {epoch} epoch')\n",
    "      # epoch=0\n",
    "      random.shuffle(common_train_index_list_k)\n",
    "\n",
    "      train_index_batches = [common_train_index_list_k[i:i + batch_size] for i in range(0, len(common_train_index_list_k), batch_size)] \n",
    "      total_loss = 0.0\n",
    "      # Iterate over the batches of the dataset.\n",
    "      for step, batch_index in enumerate(train_index_batches):\n",
    "          \n",
    "          partial_grads = client1.next_batch(batch_index)\n",
    "          client2.next_batch(batch_index)\n",
    "\n",
    "          prob, loss_value = client2.loss_and_update(client1.cal_model())\n",
    "          grad = client2.assemble_grad(partial_grads)\n",
    "          client1.update_with(grad)\n",
    "          \n",
    "          total_loss = loss_value + total_loss\n",
    "          train_acc_metric.update_state(client2.batch_answers(), prob)\n",
    "      train_acc = train_acc_metric.result()\n",
    "      print(f'===train accuracy{train_acc}====loss:{total_loss/(step + 1)}')\n",
    "      train_acc_metric.reset_states()\n",
    "      epoch_loss.append((total_loss)/(step + 1))\n",
    "      epoch_acc.append(train_acc)\n",
    "\n",
    "  plot_loss(epoch_loss, epoch_acc)\n",
    "\n",
    "  # 預測結果\n",
    "  vfl_pred_test = (client1.predict(common_test_index_k) + client2.predict(common_test_index_k))/2\n",
    "\n",
    "  # 計算roc,auc\n",
    "  vfl_fpr_test, vfl_tpr_test, vfl_thresholds_test = roc_curve(client2.test_answers(common_test_index_k), vfl_pred_test[:,1])\n",
    "  auc1 = auc(vfl_fpr_test, vfl_tpr_test)\n",
    "  draw_roc_curve(vfl_fpr_test, vfl_tpr_test,auc1)\n",
    "  print(\"AUC: {}\".format(auc1 ))\n",
    "\n",
    "  # 計算threshold 值\n",
    "  vfl_gmeans_test = np.sqrt(vfl_tpr_test * (1-vfl_fpr_test))\n",
    "  vfl_ix_test = np.argmax(vfl_gmeans_test)\n",
    "  best_threshold = vfl_thresholds_test[vfl_ix_test]\n",
    "  print('Best Threshold=%f, G-Mean=%.3f\\n' % (vfl_thresholds_test[vfl_ix_test], vfl_gmeans_test[vfl_ix_test]))\n",
    "\n",
    "  # 準確率\n",
    "\n",
    "  accuracy, precision, recall, fmeasure=plot_accuracy(vfl_pred_test, client2.test_answers(common_test_index_k), best_threshold)\n",
    "\n",
    "\n",
    "  # save result\n",
    "  df_result = df_result.append({'model':'vfl','fold':i,'accuracy':accuracy,'precision':precision,'recall':recall,'fmeasure':fmeasure},ignore_index=True)\n",
    "  #evalueate\n",
    "df_result.to_csv('vfl_score.csv',index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ----------------------------------centralized----------------------------"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 設定參數\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "\n",
    "# Instantiate a metric function (accuracy)\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "df_result = pd.DataFrame(columns=['model','fold','accuracy','precision','recall','fmeasure'])\n",
    "fold_size = len(cv_common_index ) // num_folds\n",
    "for i in range(num_folds):\n",
    "  test_start = i * fold_size\n",
    "  test_end = test_start + fold_size\n",
    "  \n",
    "  test_common_index_k = cv_common_index[test_start:test_end]\n",
    "  \n",
    "  train_common_index_k = np.concatenate((cv_common_index[:test_start], cv_common_index[test_end:]))\n",
    "  \n",
    "\n",
    "  print(f'------this is {i}/{num_folds} fold------')\n",
    "  # index\n",
    "  cen1_train_x_index = train_common_index_k\n",
    "  cen2_train_x_index = train_common_index_k\n",
    "  \n",
    "  cen1_test_x_index = test_common_index_k\n",
    "  cen2_test_x_index = test_common_index_k\n",
    "  \n",
    "  cen1_train_y_index = train_common_index_k\n",
    "  cen2_train_y_index = train_common_index_k\n",
    "  \n",
    "  cen1_test_y_index = test_common_index_k\n",
    "  cen2_test_y_index = test_common_index_k\n",
    "  \n",
    "  \n",
    "  # define train,test x use index\n",
    "  client1_train_x_k = client1_input[client1_input.index.isin(cen1_train_x_index)]\n",
    "  client2_train_x_k = client2_input[client2_input.index.isin(cen2_train_x_index)]\n",
    "  \n",
    "  client1_test_x_k = client1_input[client1_input.index.isin(cen1_test_x_index)]\n",
    "  client2_test_x_k = client2_input[client2_input.index.isin(cen2_test_x_index)]\n",
    "\n",
    "  client1_train_y_k = client1_target[client1_target.index.isin(cen1_train_y_index)]\n",
    "  client2_train_y_k = client2_target[client2_target.index.isin(cen2_train_y_index)]\n",
    "  \n",
    "  client1_test_y_k = client1_target[client1_target.index.isin(cen1_test_y_index)]\n",
    "  client2_test_y_k = client2_target[client2_target.index.isin(cen2_test_y_index)]\n",
    "  \n",
    "\n",
    "  common_train_index_k = np.intersect1d(cen1_train_x_index,cen2_train_x_index)\n",
    "  common_test_index_k = np.intersect1d(cen1_test_x_index,cen2_test_x_index)\n",
    "  \n",
    "  # ----------init cen_1\n",
    "  normalizer_cen1 = normalize_data(client1_train_x_k.loc[common_train_index_k])\n",
    "  model_cen1 = tf.keras.Sequential([\n",
    "        normalizer_cen1,\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2),\n",
    "        layers.Softmax()])\n",
    "\n",
    "\n",
    "  epoch_loss=[]\n",
    "  epoch_acc=[]\n",
    "  # custom callback\n",
    "  class PrintMetricsCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs=None):\n",
    "          epoch_loss.append(logs[\"loss\"])\n",
    "          epoch_acc.append(logs[\"accuracy\"])\n",
    "          print(f'Epoch {epoch+1}: Loss={logs[\"loss\"]:.4f}, Accuracy={logs[\"accuracy\"]:.4f}')\n",
    "\n",
    "\n",
    "  model_cen1.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  # fit1\n",
    "  test_results = {}\n",
    "  cen1_history = model_cen1.fit(client1_train_x_k, client1_train_y_k, epochs=epochs, verbose=0, batch_size=batch_size, callbacks=[PrintMetricsCallback()])\n",
    "\n",
    "\n",
    "  #evaluate\n",
    "  test_loss, test_acc = model_cen1.evaluate(client1_test_x_k, client1_test_y_k, verbose=2)\n",
    "  print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "\n",
    "  # result\n",
    "  cen1_pred_test= model_cen1.predict(client1_test_x_k)\n",
    "\n",
    "  # plot loss\n",
    "  plot_loss(epoch_loss, epoch_acc)\n",
    "  # 計算roc,auc\n",
    "  cen1_fpr_test, cen1_tpr_test, cen1_thresholds_test = roc_curve(client1_test_y_k, cen1_pred_test[:,1])\n",
    "  auc_cen1 = auc(cen1_fpr_test, cen1_tpr_test)\n",
    "  draw_roc_curve(cen1_fpr_test, cen1_tpr_test,auc=auc_cen1)\n",
    "  print(\"AUC: {}\".format(auc_cen1 ))\n",
    "\n",
    "  # 計算threshold 值\n",
    "\n",
    "  cen1_gmeans_test = np.sqrt(cen1_tpr_test * (1-cen1_fpr_test))\n",
    "  cen1_ix_test = np.argmax(cen1_gmeans_test)\n",
    "  best_threshold_cen1 = cen1_thresholds_test[cen1_ix_test]\n",
    "  print('Best Threshold=%f, G-Mean=%.3f\\n' % (cen1_thresholds_test[cen1_ix_test], cen1_gmeans_test[cen1_ix_test]))\n",
    "\n",
    "\n",
    "  # probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "  accuracy, precision, recall, fmeasure=plot_accuracy(cen1_pred_test,client1_test_y_k,best_threshold_cen1)\n",
    "  cen1_pred_test_label = [1 if p >= best_threshold_cen1 else 0 for p in  cen1_pred_test[:,1]]\n",
    "  # df['predict_cen1']=cen1_pred_test_label\n",
    "  # df.to_csv('vfl_cen_predict.csv',encoding ='UTF-8-sig')\n",
    "  df_result = df_result.append({'model':'cen1','fold':i,'accuracy':accuracy,'precision':precision,'recall':recall,'fmeasure':fmeasure},ignore_index=True)\n",
    "\n",
    "# ---------------init cen_2\n",
    "  normalizer_cen2 = normalize_data(client2_train_x_k.loc[common_train_index_k])\n",
    "  model_cen2 = tf.keras.Sequential([\n",
    "        normalizer_cen2,\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2),\n",
    "        layers.Softmax()])\n",
    "\n",
    "  epoch_loss=[]\n",
    "  epoch_acc=[]\n",
    "  # custom callback\n",
    "  class PrintMetricsCallback(tf.keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs=None):\n",
    "          epoch_loss.append(logs[\"loss\"])\n",
    "          epoch_acc.append(logs[\"accuracy\"])\n",
    "          print(f'Epoch {epoch+1}: Loss={logs[\"loss\"]:.4f}, Accuracy={logs[\"accuracy\"]:.4f}')\n",
    "\n",
    "\n",
    "  model_cen2.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  # fit2\n",
    "  test_results = {}\n",
    "  cen2_history = model_cen2.fit(client2_train_x_k, client2_train_y_k, epochs=epochs, verbose=0, batch_size=batch_size, callbacks=[PrintMetricsCallback()])\n",
    "\n",
    "\n",
    "  #evaluate\n",
    "  test_loss, test_acc = model_cen2.evaluate(client2_test_x_k, client2_test_y_k, verbose=2)\n",
    "  print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "\n",
    "  # result\n",
    "  cen2_pred_test= model_cen2.predict(client2_test_x_k)\n",
    "\n",
    "  # plot loss\n",
    "  plot_loss(epoch_loss, epoch_acc)\n",
    "\n",
    "  # 計算roc,auc\n",
    "  cen2_fpr_test, cen2_tpr_test, cen2_thresholds_test = roc_curve(client2_test_y_k, cen2_pred_test[:,1])\n",
    "  auc_cen2 = auc(cen2_fpr_test, cen2_tpr_test)\n",
    "  draw_roc_curve(cen2_fpr_test, cen2_tpr_test,auc=auc_cen2)\n",
    "  print(\"AUC: {}\".format(auc_cen2 ))\n",
    "\n",
    "  # 計算threshold 值\n",
    "\n",
    "  cen2_gmeans_test = np.sqrt(cen2_tpr_test * (2-cen2_fpr_test))\n",
    "  cen2_ix_test = np.argmax(cen2_gmeans_test)\n",
    "  best_threshold_cen2 = cen2_thresholds_test[cen2_ix_test]\n",
    "  print('Best Threshold=%f, G-Mean=%.3f\\n' % (cen2_thresholds_test[cen2_ix_test], cen2_gmeans_test[cen2_ix_test]))\n",
    "\n",
    "  # save result\n",
    "  # probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "  accuracy, precision, recall, fmeasure=plot_accuracy(cen2_pred_test,client2_test_y_k,best_threshold_cen2)\n",
    "  cen2_pred_test_label = [1 if p >= best_threshold_cen2 else 0 for p in  cen2_pred_test[:,1]]\n",
    "  # df['predict_cen2']=cen2_pred_test_label\n",
    "  # df.to_csv('vfl_cen_predict.csv',encoding ='UTF-8-sig')\n",
    "  df_result = df_result.append({'model':'cen2','fold':i,'accuracy':accuracy,'precision':precision,'recall':recall,'fmeasure':fmeasure},ignore_index=True)\n",
    "\n",
    "df_result.to_csv('cen12_score.csv',encoding ='UTF-8-sig')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}
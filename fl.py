# -*- coding: utf-8 -*-
"""fl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17RE_Ia7U6mejhWM2JGMJXU0t4WIyLsmD

平均測試誤差（average test loss):比較模型準確率時，越低越好～
-> 數據集較少，所以結果有隨機性
"""

import torch
import torch.nn as nn
import torch.optim as optim

# 定義線性回歸模型
class LinearRegression(nn.Module):
    def __init__(self, input_size):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_size, 1)

    def forward(self, x):
        out = self.linear(x)
        return out

# 數據
data = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]])
target = torch.tensor([[2.0], [5.0], [8.0], [11.0]])
train_data = data[:2]
train_target = target[:2]
test_data = data[2:]
test_target = target[2:]

# 訓練
model = LinearRegression(train_data.shape[1])
optimizer = optim.SGD(model.parameters(), lr=0.01)
for i in range(100):
    optimizer.zero_grad()
    output = model(train_data)
    loss = nn.MSELoss()(output, train_target)
    loss.backward()
    optimizer.step()

# 計算平均誤差
model.eval()
with torch.no_grad():
    output = model(test_data)
    loss = nn.MSELoss()(output, test_target)
test_loss = loss.item()
print("非fl平均測試誤差:", test_loss)

import torch
import torch.nn as nn
import torch.optim as optim

# 定義線性回歸模型
class LinearRegression(nn.Module):
    def __init__(self, input_size):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_size, 1)

    def forward(self, x):
        out = self.linear(x)
        return out

# 客戶端
class Client:
    def __init__(self, data, target):
        self.model = LinearRegression(data.shape[1])
        self.data = data
        self.target = target
        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001)

    def train(self):
        self.optimizer.zero_grad()
        output = self.model(self.data)
        loss = nn.MSELoss()(output, self.target)
        loss.backward()
        self.optimizer.step()
        #print("client")
        return self.model.state_dict()
    def test(self):
        self.model.eval()
        with torch.no_grad():
            # 預測測試資料
            output = self.model(self.data)
            # 計算均方誤差
            loss = nn.MSELoss()(output, self.target)
        return loss.item()

# 服務器
class Server:
    def __init__(self, clients):
        self.clients = clients

    def aggregate(self, state_dicts):
        # 聚合參數
        averaged_dict = {}
        for k in state_dicts[0].keys():
            averaged_dict[k] = torch.stack([state_dicts[i][k] for i in range(len(state_dicts))]).mean(0)
        return averaged_dict

    def train(self):
        # 從每個客戶端搜集參數
        state_dicts = [client.train() for client in self.clients]
        # 聚合參數並更新服務器模型
        self.model.load_state_dict(self.aggregate(state_dicts))
        #print("server")

# 數據集（兩個client)
data = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]])
target = torch.tensor([[2.0], [5.0], [8.0], [11.0]])
client1 = Client(data[:2], target[:2])
client2 = Client(data[2:], target[2:])
#print("client1：","\n",client1.data,client1.target)
#print("client2：","\n",client2.data,client2.target)

# 初始化服務器並訓練
server = Server([client1, client2])
server.model = LinearRegression(data.shape[1])
for i in range(100):
    server.train()

# 評估模型
test_loss1 = client1.test()
test_loss2 = client2.test()
#print(test_loss2)
avg_test_loss = (test_loss1 + test_loss2) / 2
print("fl平均測試誤差:", avg_test_loss)

# 忘記當初是在修正什麼error
# !pip install tensorflow-privacy
# !pip install --upgrade scipy
# !pip install pyinstaller

# 搞一些插件
# !pip install h5py
# !pip install typing-extensions
# !pip install wheel